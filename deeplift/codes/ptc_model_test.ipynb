{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "saved_model_file = 'ptc_model.h5'\n",
    "model = load_model(saved_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_classes = 2\n",
    "path_root = \"/home/muskaanjain/ntu/Muskaan/graph_2D_CNN/datasets/\"\n",
    "dataset = \"test\"\n",
    "definition = 14\n",
    "n_channels = 5\n",
    "dim_ordering = 'th' # channels first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== loading labels ==========\n",
      "classes: [1, -1]\n",
      "converting to 0-based index\n",
      "classes: [0, 1]\n",
      "========== loading tensors ==========\n",
      "210 90\n",
      "['0.npy', '1.npy', '2.npy', '3.npy', '4.npy'] ['0.npy', '1.npy', '2.npy', '3.npy', '4.npy']\n",
      "['205.npy', '206.npy', '207.npy', '208.npy', '209.npy'] ['85.npy', '86.npy', '87.npy', '88.npy', '89.npy']\n",
      "True\n",
      "converting labels to array\n",
      "transforming integer labels into one-hot vectors\n"
     ]
    }
   ],
   "source": [
    "from os.path import dirname, abspath, join\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "\n",
    "sys.path.append('/home/muskaanjain/ntu')\n",
    "\n",
    "print('========== loading labels ==========')\n",
    "\n",
    "with open(path_root + 'classes/' + dataset + '/' + dataset + '_classes.txt', 'r') as f:\n",
    "    ys = f.read().splitlines()\n",
    "    ys = [int(elt) for elt in ys]\n",
    "\n",
    "num_classes = len(list(set(ys)))\n",
    "print('classes:', list(set(ys)))\n",
    "\n",
    "print('converting to 0-based index')\n",
    "\n",
    "if 0 not in list(set(ys)):\n",
    "    if -1 not in list(set(ys)):\n",
    "        ys = [y-1 for y in ys]\n",
    "    else:\n",
    "        ys = [1 if y==1 else 0 for y in ys]\n",
    "\n",
    "print('classes:', list(set(ys)) )\n",
    "\n",
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "\n",
    "def natural_keys(text):\n",
    "    return [atoi(c) for c in re.split('(\\d+)', text)]\n",
    "\n",
    "print('========== loading tensors ==========')\n",
    "\n",
    "train_path_read = path_root + 'hist/' + 'train' + '/embed_hist/'\n",
    "test_path_read = path_root + 'hist/' + 'test' + '/embed_hist/'\n",
    "\n",
    "train_file_names = [elt for elt in os.listdir(train_path_read)] # make sure the right files are selected\n",
    "test_file_names = [elt for elt in os.listdir(test_path_read)] # make sure the right files are selected\n",
    "train_file_names.sort(key=natural_keys)\n",
    "test_file_names.sort(key=natural_keys)\n",
    "\n",
    "print(len(train_file_names), len(test_file_names))\n",
    "print(train_file_names[:5], test_file_names[:5])\n",
    "print(train_file_names[-5:], test_file_names[-5:])\n",
    "\n",
    "ys = [y for idx,y in enumerate(ys)]\n",
    "print(len(train_file_names) + len(test_file_names) == len(ys))\n",
    "\n",
    "print('converting labels to array')\n",
    "ys = np.array(ys)\n",
    "\n",
    "print('transforming integer labels into one-hot vectors')\n",
    "ys = np_utils.to_categorical(ys, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train tensors shape: (210, 131, 131, 5)\n",
      "test tensors shape: (90, 131, 131, 5)\n",
      "========== getting image dimensions ==========\n",
      "input shape: (131, 131, 5)\n",
      "========== shuffling data ==========\n"
     ]
    }
   ],
   "source": [
    "train_tensors = []\n",
    "test_tensors = []\n",
    "\n",
    "for name in train_file_names:\n",
    "    tensor = np.load(train_path_read + name)\n",
    "    train_tensors.append(tensor[:n_channels,:,:])\n",
    "    \n",
    "for name in test_file_names:\n",
    "    tensor = np.load(test_path_read + name)\n",
    "    test_tensors.append(tensor[:n_channels,:,:])\n",
    "    \n",
    "train_tensors = np.array(train_tensors)\n",
    "train_tensors = train_tensors.astype('float32')\n",
    "train_tensors = np.rollaxis(train_tensors, 3, 1)\n",
    "train_tensors = np.rollaxis(train_tensors, 3, 1)\n",
    "\n",
    "test_tensors = np.array(test_tensors)\n",
    "test_tensors = test_tensors.astype('float32')\n",
    "test_tensors = np.rollaxis(test_tensors, 3, 1)\n",
    "test_tensors = np.rollaxis(test_tensors, 3, 1)\n",
    "\n",
    "print('train tensors shape:', train_tensors.shape)\n",
    "print('test tensors shape:', test_tensors.shape)\n",
    "\n",
    "\n",
    "print('========== getting image dimensions ==========')\n",
    "\n",
    "img_rows, img_cols = int(train_tensors.shape[2]), int(train_tensors.shape[3])\n",
    "input_shape = (int(train_tensors.shape[1]), img_rows, img_cols)   \n",
    "\n",
    "print('input shape:', input_shape )\n",
    "\n",
    "print('========== shuffling data ==========')\n",
    "\n",
    "saved_dataset_idxs = np.load('/home/muskaanjain/ntu/Muskaan/gspannew/shuffled_graph_idxs.npy')\n",
    "ys = ys[saved_dataset_idxs.tolist()]\n",
    "\n",
    "#shuffled_idxs = random.sample(range(tensors.shape[0]), int(tensors.shape[0])) # sample w/o replct\n",
    "#tensors = tensors[shuffled_idxs]\n",
    "#ys = ys[shuffled_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "train_test_split = 0.7\n",
    "split_point = (int)(len(train_tensors))\n",
    "x_train = np.array(train_tensors)\n",
    "x_test = np.array(test_tensors)\n",
    "        \n",
    "y_train = np.array(ys[:split_point])\n",
    "y_test = np.array(ys[split_point:])\n",
    "\n",
    "print(len(x_train) == len(y_train))\n",
    "print(len(x_test) == len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 1s 16ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3733558575312297, 0.600000003973643]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 129, 129, 64)      2944      \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 129, 129, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 62, 62, 96)        55392     \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 62, 62, 96)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 31, 31, 96)        0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 31, 31, 96)        0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 92256)             0         \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 92256)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 128)               11808896  \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 11,867,490\n",
      "Trainable params: 11,867,490\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nonlinear_mxts_mode is set to: RevealCancel\n",
      "131\n",
      "129\n",
      "Heads-up: current implementation assumes maxpool layer is followed by a linear transformation (conv/dense layer)\n",
      "65\n",
      "63\n",
      "Heads-up: current implementation assumes maxpool layer is followed by a linear transformation (conv/dense layer)\n",
      "Heads-up: I assume softmax is the output layer, not an intermediate one; if it's an intermediate layer, please let me know and I will prioritise that use-case\n"
     ]
    }
   ],
   "source": [
    "sys.path.append('/home/muskaanjain/git/deeplift')\n",
    "\n",
    "import deeplift\n",
    "from deeplift.layers import NonlinearMxtsMode\n",
    "from deeplift.conversion import kerasapi_conversion as kc\n",
    "\n",
    "revealcancel_model = kc.convert_model_from_saved_files(\n",
    "                            h5_file=saved_model_file,\n",
    "                            nonlinear_mxts_mode=NonlinearMxtsMode.RevealCancel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "difference in predictions: 0.0\n"
     ]
    }
   ],
   "source": [
    "from deeplift.util import compile_func\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "\n",
    "deeplift_model = revealcancel_model\n",
    "deeplift_prediction_func = compile_func([deeplift_model.get_layers()[0].get_activation_vars()],\n",
    "                                       deeplift_model.get_layers()[-1].get_activation_vars())\n",
    "original_model_predictions = model.predict(x_test, batch_size=32)\n",
    "converted_model_predictions = deeplift.util.run_function_in_batches(\n",
    "                                input_data_list=[x_test],\n",
    "                                func=deeplift_prediction_func,\n",
    "                                batch_size=32,\n",
    "                                progress_update=None)\n",
    "print(\"difference in predictions:\",np.max(np.array(converted_model_predictions)-np.array(original_model_predictions)))\n",
    "assert np.max(np.array(converted_model_predictions)-np.array(original_model_predictions)) < 10**-5\n",
    "predictions = converted_model_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<deeplift.layers.core.Dense object at 0x7fee313e3278>\n",
      "2\n",
      "OneAndZeros\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "import deeplift\n",
    "from deeplift.util import get_integrated_gradients_function\n",
    "\n",
    "revealcancel_func = revealcancel_model.get_target_contribs_func(find_scores_layer_idx=0, target_layer_idx=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing scores for: revealcancel\n",
      "\tComputing scores for task: 0\n",
      "\tComputing scores for task: 1\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "method_to_task_to_scores = OrderedDict()\n",
    "\n",
    "for method_name, score_func in [('revealcancel', revealcancel_func)]:\n",
    "    print(\"Computing scores for:\",method_name)\n",
    "    method_to_task_to_scores[method_name] = {}\n",
    "    for task_idx in range(num_of_classes):\n",
    "        print(\"\\tComputing scores for task: \"+str(task_idx))\n",
    "        scores = np.array(score_func(\n",
    "                    task_idx=task_idx,\n",
    "                    input_data_list=[x_test],\n",
    "                    input_references_list=[np.zeros_like(x_test)],\n",
    "                    batch_size=100,\n",
    "                    progress_update=None))\n",
    "        method_to_task_to_scores[method_name][task_idx] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90, 131, 131, 5)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 5, 131, 131)\n",
      "(90, 131, 131, 5)\n",
      "(210, 5, 131, 131)\n",
      "(210, 131, 131, 5)\n"
     ]
    }
   ],
   "source": [
    "h_test = [] #corresponds to 5*131*131\n",
    "graph_files  = os.listdir('/home/muskaanjain/ntu/version1/graphimg_with_ids/test/') \n",
    "for idx, name in enumerate(graph_files):\n",
    "    graph = np.load('/home/muskaanjain/ntu/version1/graphimg_with_ids/test/' + name)\n",
    "    h_test.append(graph)\n",
    "\n",
    "address_test = np.empty([len(h_test), 5, 131, 131], dtype=int)\n",
    "for graph in range(len(h_test)):\n",
    "    for channel in range(n_channels):\n",
    "        address_test[graph, channel, :, :] = (np.arange(1, (131*131)+1, 1)).reshape(131, 131)\n",
    "    \n",
    "address_test = np.array(address_test)\n",
    "print(np.array(address_test).shape)\n",
    "address_test = np.rollaxis(address_test, 3, 1)\n",
    "address_test = np.rollaxis(address_test, 3, 1)\n",
    "print(np.array(address_test).shape)\n",
    "\n",
    "\n",
    "h_train = [] #corresponds to 5*131*131\n",
    "graph_files  = os.listdir('/home/muskaanjain/ntu/version1/graphimg_with_ids/train/') \n",
    "for idx, name in enumerate(graph_files):\n",
    "    graph = np.load('/home/muskaanjain/ntu/version1/graphimg_with_ids/train/' + name)\n",
    "    h_train.append(graph)\n",
    "\n",
    "address_train = np.empty([len(h_train), 5, 131, 131], dtype=int)\n",
    "for graph in range(len(h_train)):\n",
    "    for channel in range(n_channels):\n",
    "        address_train[graph, channel, :, :] = (np.arange(1, (131*131)+1, 1)).reshape(131, 131)\n",
    "    \n",
    "address_train = np.array(address_train)\n",
    "print(np.array(address_train).shape)\n",
    "address_train = np.rollaxis(address_train, 3, 1)\n",
    "address_train = np.rollaxis(address_train, 3, 1)\n",
    "print(np.array(address_train).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check :\n",
      "True\n",
      "0 predictions:: right :  32  wrong :  17\n",
      "1 predictions:: right :  22  wrong :  19\n"
     ]
    }
   ],
   "source": [
    "#Store the image indices which are predicted right or wrong separately for cats and dogs.\n",
    "y_pred = (np.array(converted_model_predictions) >= 0.5)*1 + (np.array(converted_model_predictions) < 0.5)*0\n",
    "predictions = np.argmax(y_pred, axis=1)\n",
    "original = np.argmax(y_test, axis=1)\n",
    "\n",
    "right_0_pred_idx = []\n",
    "wrong_0_pred_idx = []\n",
    "right_1_pred_idx = []\n",
    "wrong_1_pred_idx = []\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    if(original[i] == 0):\n",
    "        if(predictions[i] == 0):\n",
    "            right_0_pred_idx.append(i)\n",
    "        else:\n",
    "            wrong_0_pred_idx.append(i)\n",
    "    else:\n",
    "        if(predictions[i] == 1):\n",
    "            right_1_pred_idx.append(i)\n",
    "        else:\n",
    "            wrong_1_pred_idx.append(i)\n",
    "            \n",
    "print(\"Sanity check :\")\n",
    "print((len(right_0_pred_idx) + len(right_1_pred_idx) + len(wrong_0_pred_idx) + len(wrong_1_pred_idx)) == len(y_test))\n",
    "\n",
    "print(\"0 predictions:: right : \", len(right_0_pred_idx), \" wrong : \", len(wrong_0_pred_idx))\n",
    "print(\"1 predictions:: right : \", len(right_1_pred_idx), \" wrong : \", len(wrong_1_pred_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_img_set1 = [right_0_pred_idx[4], right_0_pred_idx[10], right_0_pred_idx[2], right_0_pred_idx[0]]\n",
    "random_img_set2 = [wrong_0_pred_idx[4], wrong_0_pred_idx[10], wrong_0_pred_idx[2], wrong_0_pred_idx[0]]\n",
    "random_img_set3 = [right_1_pred_idx[7], right_1_pred_idx[17], right_1_pred_idx[2], right_1_pred_idx[3]]\n",
    "random_img_set4 = [wrong_1_pred_idx[7], wrong_1_pred_idx[17], wrong_1_pred_idx[2], wrong_1_pred_idx[3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Important subgraphs in an image: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for graph:  8\n",
      "257 :  [(0, 1, '16', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '18', '0'), (4, 5, '18', '18', '0')]\n",
      "365 :  [(0, 1, '18', '18', '0'), (1, 2, '18', '18', '0')]\n",
      "\n",
      "\n",
      "\n",
      "for graph:  40\n",
      "232 :  [(0, 1, '16', '18', '0')]\n",
      "233 :  [(0, 1, '16', '18', '0'), (1, 2, '18', '16', '0')]\n",
      "234 :  [(0, 1, '16', '18', '0'), (1, 2, '18', '16', '0'), (1, 3, '18', '16', '0')]\n",
      "13 :  [(0, 1, '11', '18', '0')]\n",
      "\n",
      "\n",
      "\n",
      "for graph:  4\n",
      "129 :  [(0, 1, '11', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (1, 4, '18', '18', '0')]\n",
      "137 :  [(0, 1, '11', '18', '0'), (0, 2, '11', '18', '0')]\n",
      "171 :  [(0, 1, '14', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '18', '0')]\n",
      "76 :  [(0, 1, '11', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '18', '0'), (4, 5, '18', '18', '0'), (5, 6, '18', '18', '0'), (6, 1, '18', '18', '0'), (5, 7, '18', '18', '0')]\n",
      "27 :  [(0, 1, '11', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '16', '0'), (2, 4, '18', '18', '0'), (1, 4, '18', '18', '0')]\n",
      "332 :  [(0, 1, '16', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '18', '0'), (4, 5, '18', '18', '0'), (5, 6, '18', '18', '0'), (4, 7, '18', '18', '0')]\n",
      "365 :  [(0, 1, '18', '18', '0'), (1, 2, '18', '18', '0')]\n",
      "409 :  [(0, 1, '18', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (2, 4, '18', '18', '0')]\n",
      "283 :  [(0, 1, '16', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '18', '0'), (4, 5, '18', '18', '0'), (5, 6, '18', '18', '0'), (6, 7, '18', '18', '0'), (7, 8, '18', '16', '0')]\n",
      "188 :  [(0, 1, '14', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '18', '0'), (4, 5, '18', '18', '0'), (5, 6, '18', '16', '0'), (1, 7, '18', '18', '0')]\n",
      "375 :  [(0, 1, '18', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '18', '0'), (4, 5, '18', '18', '0'), (5, 6, '18', '18', '0')]\n",
      "121 :  [(0, 1, '11', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '18', '0'), (3, 5, '18', '18', '0')]\n",
      "87 :  [(0, 1, '11', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '18', '0'), (4, 5, '18', '18', '0'), (5, 6, '18', '18', '0'), (6, 7, '18', '16', '0'), (0, 8, '11', '18', '0')]\n",
      "251 :  [(0, 1, '16', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '18', '0')]\n",
      "252 :  [(0, 1, '16', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '18', '0'), (4, 5, '18', '16', '0')]\n",
      "29 :  [(0, 1, '11', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '16', '0'), (2, 4, '18', '18', '0'), (0, 4, '11', '18', '0')]\n",
      "\n",
      "\n",
      "\n",
      "for graph:  0\n",
      "287 :  [(0, 1, '16', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '18', '0'), (4, 5, '18', '18', '0'), (5, 6, '18', '18', '0'), (6, 7, '18', '18', '0'), (7, 8, '18', '16', '0'), (4, 9, '18', '18', '0')]\n",
      "375 :  [(0, 1, '18', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '18', '0'), (4, 5, '18', '18', '0'), (5, 6, '18', '18', '0')]\n",
      "391 :  [(0, 1, '18', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '18', '0'), (4, 5, '18', '18', '0'), (5, 6, '18', '18', '0'), (5, 7, '18', '18', '0')]\n",
      "171 :  [(0, 1, '14', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '18', '0')]\n",
      "364 :  [(0, 1, '18', '18', '0')]\n",
      "365 :  [(0, 1, '18', '18', '0'), (1, 2, '18', '18', '0')]\n",
      "368 :  [(0, 1, '18', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '18', '0'), (4, 5, '18', '18', '0')]\n",
      "369 :  [(0, 1, '18', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '18', '0'), (4, 5, '18', '18', '0'), (5, 0, '18', '18', '0')]\n",
      "144 :  [(0, 1, '14', '18', '0'), (1, 2, '18', '16', '0')]\n",
      "180 :  [(0, 1, '14', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '18', '0'), (4, 5, '18', '18', '0')]\n",
      "148 :  [(0, 1, '14', '18', '0'), (1, 2, '18', '18', '0')]\n",
      "404 :  [(0, 1, '18', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '18', '0'), (2, 5, '18', '18', '0')]\n",
      "215 :  [(0, 1, '14', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '18', '0'), (2, 5, '18', '18', '0')]\n",
      "409 :  [(0, 1, '18', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (2, 4, '18', '18', '0')]\n",
      "188 :  [(0, 1, '14', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '18', '0'), (4, 5, '18', '18', '0'), (5, 6, '18', '16', '0'), (1, 7, '18', '18', '0')]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "ix = 0\n",
    "n_task1 = 15\n",
    "\n",
    "for i in random_img_set1:\n",
    "    s = method_to_task_to_scores['revealcancel'][0][i]\n",
    "    arr = s.ravel()\n",
    "    top_nth_threshold = max(sorted(arr, reverse=True)[n_task1],0.0) #can be done before smoothing the scores\n",
    "    thresholded_points = 1.0*(arr >= top_nth_threshold)\n",
    "    thresholded_points = thresholded_points.reshape(131, 131, 5) #why 5 channels?\n",
    "    imp_inp = thresholded_points*x_test[i]\n",
    "    imp_subgraphs = thresholded_points*address_test[i]\n",
    "    #top_ones = imp_subgraphs[np.nonzero(imp_subgraphs)]\n",
    "    top_ones = []\n",
    "    for channel in range(n_channels):\n",
    "        a = imp_subgraphs[:, :, channel].ravel()\n",
    "        nonzeros = a[np.nonzero(a)]\n",
    "        nonzeros = nonzeros.astype(int)\n",
    "        for x in nonzeros:\n",
    "                top_ones += h[i][channel][x-1]\n",
    "    top_ones = set(top_ones)      \n",
    "    extensions = np.load('/home/muskaanjain/ntu/version1/extensions.npy')\n",
    "    extensions = np.array(extensions)\n",
    "    print(\"for graph: \", i)\n",
    "    for j in top_ones:\n",
    "        print(j-1, \": \", extensions[int(j-1)])\n",
    "    \n",
    "    print(\"\\n\\n\")\n",
    "    ix += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for graph:  31\n",
      "226 :  [(0, 1, '14', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (0, 4, '14', '18', '0')]\n",
      "169 :  [(0, 1, '14', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '16', '0'), (0, 5, '14', '18', '0')]\n",
      "138 :  [(0, 1, '14', '16', '0')]\n",
      "171 :  [(0, 1, '14', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '18', '0')]\n",
      "141 :  [(0, 1, '14', '16', '0'), (0, 2, '14', '18', '0')]\n",
      "50 :  [(0, 1, '11', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '18', '0'), (4, 5, '18', '16', '0'), (4, 6, '18', '18', '0'), (3, 6, '18', '16', '0')]\n",
      "181 :  [(0, 1, '14', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '18', '0'), (4, 5, '18', '18', '0'), (5, 6, '18', '16', '0')]\n",
      "150 :  [(0, 1, '14', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '16', '0'), (2, 4, '18', '18', '0')]\n",
      "\n",
      "\n",
      "\n",
      "for graph:  81\n",
      "34 :  [(0, 1, '11', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '16', '0')]\n",
      "39 :  [(0, 1, '11', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '16', '0'), (3, 5, '18', '18', '0'), (1, 5, '18', '18', '0')]\n",
      "232 :  [(0, 1, '16', '18', '0')]\n",
      "265 :  [(0, 1, '16', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '18', '0'), (4, 5, '18', '18', '0'), (5, 6, '18', '18', '0'), (6, 1, '18', '18', '0')]\n",
      "360 :  [(0, 1, '16', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (1, 4, '18', '18', '0'), (2, 3, '18', '11', '0')]\n",
      "363 :  [(0, 1, '16', '18', '0'), (1, 2, '18', '18', '0'), (1, 3, '18', '18', '0')]\n",
      "235 :  [(0, 1, '16', '18', '0'), (1, 2, '18', '16', '0'), (1, 3, '18', '16', '0'), (1, 3, '18', '18', '0')]\n",
      "234 :  [(0, 1, '16', '18', '0'), (1, 2, '18', '16', '0'), (1, 3, '18', '16', '0')]\n",
      "241 :  [(0, 1, '16', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '16', '0'), (2, 4, '18', '18', '0')]\n",
      "118 :  [(0, 1, '11', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '18', '0'), (4, 5, '18', '18', '0'), (1, 6, '18', '18', '0'), (2, 3, '18', '16', '0')]\n",
      "350 :  [(0, 1, '16', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '18', '0'), (3, 5, '18', '18', '0')]\n",
      "\n",
      "\n",
      "\n",
      "for graph:  11\n",
      "\n",
      "\n",
      "\n",
      "for graph:  15\n",
      "65 :  [(0, 1, '11', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '18', '0'), (4, 5, '18', '18', '0'), (5, 6, '18', '16', '0'), (5, 7, '18', '18', '0'), (3, 7, '18', '16', '0')]\n",
      "355 :  [(0, 1, '16', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '18', '0'), (1, 5, '18', '18', '0'), (2, 3, '18', '11', '0')]\n",
      "37 :  [(0, 1, '11', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '16', '0'), (3, 5, '18', '18', '0'), (2, 5, '18', '16', '0')]\n",
      "39 :  [(0, 1, '11', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '16', '0'), (3, 5, '18', '18', '0'), (1, 5, '18', '18', '0')]\n",
      "360 :  [(0, 1, '16', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (1, 4, '18', '18', '0'), (2, 3, '18', '11', '0')]\n",
      "363 :  [(0, 1, '16', '18', '0'), (1, 2, '18', '18', '0'), (1, 3, '18', '18', '0')]\n",
      "11 :  [(0, 1, '11', '16', '0')]\n",
      "237 :  [(0, 1, '16', '18', '0'), (1, 2, '18', '16', '0'), (1, 3, '18', '18', '0'), (1, 3, '18', '11', '0')]\n",
      "16 :  [(0, 1, '11', '18', '0'), (1, 2, '18', '16', '0')]\n",
      "49 :  [(0, 1, '11', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '18', '0'), (4, 5, '18', '16', '0'), (4, 6, '18', '18', '0')]\n",
      "242 :  [(0, 1, '16', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '16', '0'), (2, 4, '18', '18', '0'), (1, 4, '18', '18', '0')]\n",
      "50 :  [(0, 1, '11', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '18', '0'), (4, 5, '18', '16', '0'), (4, 6, '18', '18', '0'), (3, 6, '18', '16', '0')]\n",
      "48 :  [(0, 1, '11', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '18', '0'), (4, 5, '18', '16', '0'), (4, 6, '18', '16', '0')]\n",
      "240 :  [(0, 1, '16', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '16', '0')]\n",
      "118 :  [(0, 1, '11', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '18', '0'), (4, 5, '18', '18', '0'), (1, 6, '18', '18', '0'), (2, 3, '18', '16', '0')]\n",
      "241 :  [(0, 1, '16', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '16', '0'), (2, 4, '18', '18', '0')]\n",
      "245 :  [(0, 1, '16', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '16', '0'), (3, 5, '18', '18', '0')]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "ix = 0\n",
    "n_task1 = 15\n",
    "\n",
    "for i in random_img_set3:\n",
    "    s = method_to_task_to_scores['revealcancel'][1][i]\n",
    "    arr = s.ravel()\n",
    "    top_nth_threshold = max(sorted(arr, reverse=True)[n_task1],0.0) #can be done before smoothing the scores\n",
    "    thresholded_points = 1.0*(arr >= top_nth_threshold)\n",
    "    thresholded_points = thresholded_points.reshape(131, 131, 5) #why 5 channels?\n",
    "    imp_inp = thresholded_points*x_test[i]\n",
    "    imp_subgraphs = thresholded_points*address_test[i]\n",
    "    #top_ones = imp_subgraphs[np.nonzero(imp_subgraphs)]\n",
    "    top_ones = []\n",
    "    for channel in range(n_channels):\n",
    "        a = imp_subgraphs[:, :, channel].ravel()\n",
    "        nonzeros = a[np.nonzero(a)]\n",
    "        nonzeros = nonzeros.astype(int)\n",
    "        for x in nonzeros:\n",
    "                top_ones += h[i][channel][x-1]\n",
    "    top_ones = set(top_ones)      \n",
    "    \n",
    "    extensions = np.load('/home/muskaanjain/ntu/version1/extensions.npy')\n",
    "    extensions = np.array(extensions)\n",
    "    print(\"for graph: \", i)\n",
    "    for j in top_ones:\n",
    "        print(j-1, \": \", extensions[int(j-1)])\n",
    "    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    ix += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: Subgraphs important for one task than the other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for graph:  29\n",
      "383 :  [(0, 1, '18', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '18', '0'), (4, 5, '18', '18', '0'), (5, 6, '18', '18', '0'), (6, 7, '18', '18', '0'), (7, 8, '18', '18', '0'), (5, 9, '18', '18', '0')]\n",
      "286 :  [(0, 1, '16', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '18', '0'), (4, 5, '18', '18', '0'), (5, 6, '18', '18', '0'), (6, 7, '18', '18', '0'), (7, 8, '18', '16', '0'), (5, 9, '18', '18', '0')]\n",
      "5 :  [(0, 1, '3', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '18', '0')]\n",
      "6 :  [(0, 1, '3', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '18', '0'), (4, 5, '18', '18', '0')]\n",
      "241 :  [(0, 1, '16', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '16', '0'), (2, 4, '18', '18', '0')]\n",
      "350 :  [(0, 1, '16', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '18', '0'), (3, 5, '18', '18', '0')]\n",
      "\n",
      "\n",
      "\n",
      "for graph:  43\n",
      "\n",
      "\n",
      "\n",
      "for graph:  13\n",
      "259 :  [(0, 1, '16', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '18', '0'), (4, 5, '18', '18', '0'), (5, 6, '18', '16', '0'), (5, 7, '18', '18', '0')]\n",
      "264 :  [(0, 1, '16', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '18', '0'), (4, 5, '18', '18', '0'), (5, 6, '18', '18', '0')]\n",
      "265 :  [(0, 1, '16', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '18', '0'), (4, 5, '18', '18', '0'), (5, 6, '18', '18', '0'), (6, 1, '18', '18', '0')]\n",
      "363 :  [(0, 1, '16', '18', '0'), (1, 2, '18', '18', '0'), (1, 3, '18', '18', '0')]\n",
      "368 :  [(0, 1, '18', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '18', '0'), (4, 5, '18', '18', '0')]\n",
      "240 :  [(0, 1, '16', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '16', '0')]\n",
      "242 :  [(0, 1, '16', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '16', '0'), (2, 4, '18', '18', '0'), (1, 4, '18', '18', '0')]\n",
      "241 :  [(0, 1, '16', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '16', '0'), (2, 4, '18', '18', '0')]\n",
      "245 :  [(0, 1, '16', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '16', '0'), (3, 5, '18', '18', '0')]\n",
      "\n",
      "\n",
      "\n",
      "for graph:  1\n",
      "241 :  [(0, 1, '16', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '16', '0'), (2, 4, '18', '18', '0')]\n",
      "34 :  [(0, 1, '11', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '16', '0')]\n",
      "265 :  [(0, 1, '16', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '18', '0'), (4, 5, '18', '18', '0'), (5, 6, '18', '18', '0'), (6, 1, '18', '18', '0')]\n",
      "50 :  [(0, 1, '11', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '18', '0'), (4, 5, '18', '16', '0'), (4, 6, '18', '18', '0'), (3, 6, '18', '16', '0')]\n",
      "11 :  [(0, 1, '11', '16', '0')]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "ix = 0\n",
    "n_task1 = 15\n",
    "\n",
    "for i in random_img_set2:\n",
    "    s = method_to_task_to_scores['revealcancel'][1][i] - method_to_task_to_scores['revealcancel'][0][i]\n",
    "    arr = s.ravel()\n",
    "    top_nth_threshold = max(sorted(arr, reverse=True)[n_task1],0.0) #can be done before smoothing the scores\n",
    "    thresholded_points = 1.0*(arr >= top_nth_threshold)\n",
    "    thresholded_points = thresholded_points.reshape(131, 131, 5) #why 5 channels?\n",
    "    imp_inp = thresholded_points*x_test[i]\n",
    "    imp_subgraphs = thresholded_points*address_test[i]\n",
    "    #top_ones = imp_subgraphs[np.nonzero(imp_subgraphs)]\n",
    "    top_ones = []\n",
    "    for channel in range(n_channels):\n",
    "        a = imp_subgraphs[:, :, channel].ravel()\n",
    "        nonzeros = a[np.nonzero(a)]\n",
    "        nonzeros = nonzeros.astype(int)\n",
    "        for x in nonzeros:\n",
    "                top_ones += h[i][channel][x-1]\n",
    "    top_ones = set(top_ones)      \n",
    "    \n",
    "    extensions = np.load('/home/muskaanjain/ntu/version1/extensions.npy')\n",
    "    extensions = np.array(extensions)\n",
    "    print(\"for graph: \", i)\n",
    "    for j in top_ones:\n",
    "        print(j-1, \": \", extensions[int(j-1)])\n",
    "    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    ix += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for graph:  29\n",
      "257 :  [(0, 1, '16', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '18', '0'), (4, 5, '18', '18', '0')]\n",
      "292 :  [(0, 1, '16', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '18', '0'), (4, 5, '18', '18', '0'), (5, 6, '18', '18', '0'), (6, 7, '18', '18', '0'), (7, 8, '18', '18', '0'), (8, 9, '18', '16', '0'), (8, 10, '18', '18', '0')]\n",
      "327 :  [(0, 1, '16', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '18', '0'), (4, 5, '18', '18', '0'), (5, 6, '18', '18', '0'), (6, 7, '18', '18', '0'), (3, 8, '18', '18', '0')]\n",
      "328 :  [(0, 1, '16', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '18', '0'), (4, 5, '18', '18', '0'), (5, 6, '18', '18', '0'), (6, 7, '18', '18', '0'), (2, 8, '18', '18', '0')]\n",
      "332 :  [(0, 1, '16', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '18', '0'), (4, 5, '18', '18', '0'), (5, 6, '18', '18', '0'), (4, 7, '18', '18', '0')]\n",
      "365 :  [(0, 1, '18', '18', '0'), (1, 2, '18', '18', '0')]\n",
      "409 :  [(0, 1, '18', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (2, 4, '18', '18', '0')]\n",
      "\n",
      "\n",
      "\n",
      "for graph:  43\n",
      "365 :  [(0, 1, '18', '18', '0'), (1, 2, '18', '18', '0')]\n",
      "\n",
      "\n",
      "\n",
      "for graph:  13\n",
      "354 :  [(0, 1, '16', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '18', '0'), (1, 5, '18', '18', '0')]\n",
      "359 :  [(0, 1, '16', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (1, 4, '18', '18', '0')]\n",
      "364 :  [(0, 1, '18', '18', '0')]\n",
      "365 :  [(0, 1, '18', '18', '0'), (1, 2, '18', '18', '0')]\n",
      "239 :  [(0, 1, '16', '18', '0'), (1, 2, '18', '18', '0')]\n",
      "369 :  [(0, 1, '18', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '18', '0'), (4, 5, '18', '18', '0'), (5, 0, '18', '18', '0')]\n",
      "243 :  [(0, 1, '16', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0')]\n",
      "244 :  [(0, 1, '16', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '16', '0')]\n",
      "245 :  [(0, 1, '16', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '16', '0'), (3, 5, '18', '18', '0')]\n",
      "409 :  [(0, 1, '18', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (2, 4, '18', '18', '0')]\n",
      "251 :  [(0, 1, '16', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '18', '0')]\n",
      "252 :  [(0, 1, '16', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '18', '0'), (4, 5, '18', '16', '0')]\n",
      "253 :  [(0, 1, '16', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '18', '0'), (4, 5, '18', '16', '0'), (4, 6, '18', '18', '0')]\n",
      "\n",
      "\n",
      "\n",
      "for graph:  1\n",
      "257 :  [(0, 1, '16', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '18', '0'), (4, 5, '18', '18', '0')]\n",
      "129 :  [(0, 1, '11', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (1, 4, '18', '18', '0')]\n",
      "42 :  [(0, 1, '11', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '16', '0'), (1, 5, '18', '18', '0')]\n",
      "365 :  [(0, 1, '18', '18', '0'), (1, 2, '18', '18', '0')]\n",
      "252 :  [(0, 1, '16', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '18', '0'), (4, 5, '18', '16', '0')]\n",
      "30 :  [(0, 1, '11', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '16', '0'), (1, 4, '18', '18', '0')]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "ix = 0\n",
    "n_task1 = 15\n",
    "\n",
    "for i in random_img_set2:\n",
    "    s = method_to_task_to_scores['revealcancel'][0][i] - method_to_task_to_scores['revealcancel'][1][i]\n",
    "    arr = s.ravel()\n",
    "    top_nth_threshold = max(sorted(arr, reverse=True)[n_task1],0.0) #can be done before smoothing the scores\n",
    "    thresholded_points = 1.0*(arr >= top_nth_threshold)\n",
    "    thresholded_points = thresholded_points.reshape(131, 131, 5) #why 5 channels?\n",
    "    imp_inp = thresholded_points*x_test[i]\n",
    "    imp_subgraphs = thresholded_points*address_test[i]\n",
    "    #top_ones = imp_subgraphs[np.nonzero(imp_subgraphs)]\n",
    "    top_ones = []\n",
    "    for channel in range(n_channels):\n",
    "        a = imp_subgraphs[:, :, channel].ravel()\n",
    "        nonzeros = a[np.nonzero(a)]\n",
    "        nonzeros = nonzeros.astype(int)\n",
    "        for x in nonzeros:\n",
    "                top_ones += h[i][channel][x-1]\n",
    "    top_ones = set(top_ones)      \n",
    "    \n",
    "    extensions = np.load('/home/muskaanjain/ntu/version1/extensions.npy')\n",
    "    extensions = np.array(extensions)\n",
    "    print(\"for graph: \", i)\n",
    "    for j in top_ones:\n",
    "        print(j-1, \": \", extensions[int(j-1)])\n",
    "    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    ix += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important subgraphs for a class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total correctly predicted images for class 0:  32\n",
      "\n",
      "\n",
      "\n",
      "subgraph id:  239  score:  13.0\n",
      "pattern :  [(0, 1, '16', '18', '0'), (1, 2, '18', '18', '0')]\n",
      "subgraph id:  252  score:  8.0\n",
      "pattern :  [(0, 1, '16', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '18', '0'), (4, 5, '18', '16', '0')]\n",
      "subgraph id:  364  score:  12.0\n",
      "pattern :  [(0, 1, '18', '18', '0')]\n",
      "subgraph id:  365  score:  9.0\n",
      "pattern :  [(0, 1, '18', '18', '0'), (1, 2, '18', '18', '0')]\n",
      "subgraph id:  409  score:  7.0\n",
      "pattern :  [(0, 1, '18', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (2, 4, '18', '18', '0')]\n"
     ]
    }
   ],
   "source": [
    "# For class 0\n",
    "print(\"Total correctly predicted images for class 0: \", len(right_0_pred_idx))\n",
    "print(\"\\n\\n\")\n",
    "subgraphs_score = np.zeros(len(extensions))\n",
    "for img in right_0_pred_idx:\n",
    "    s = method_to_task_to_scores['revealcancel'][0][img]\n",
    "    arr = s.ravel()\n",
    "    top_nth_threshold = max(sorted(arr, reverse=True)[n_task1],0.0) #can be done before smoothing the scores\n",
    "    thresholded_points = 1.0*(arr >= top_nth_threshold)\n",
    "    thresholded_points = thresholded_points.reshape(131, 131, 5) #why 5 channels?\n",
    "    imp_inp = thresholded_points*x_test[img]\n",
    "    imp_subgraphs = thresholded_points*address_test[img]\n",
    "    #top_ones = imp_subgraphs[np.nonzero(imp_subgraphs)]\n",
    "    top_ones = []\n",
    "    for channel in range(n_channels):\n",
    "        a = imp_subgraphs[:, :, channel].ravel()\n",
    "        nonzeros = a[np.nonzero(a)]\n",
    "        nonzeros = nonzeros.astype(int)\n",
    "        for x in nonzeros:\n",
    "                top_ones += h[img][channel][x-1]\n",
    "    top_ones = set(top_ones)      \n",
    "    for top in top_ones:\n",
    "        subgraphs_score[top-1] += 1\n",
    "    ix += 1\n",
    "    \n",
    "top_percent = 1\n",
    "top_score_value = np.percentile(subgraphs_score, 100 - top_percent)\n",
    "#print(top_filters_value)\n",
    "\n",
    "top_subgraphs = []\n",
    "for i in range(len(subgraphs_score)):\n",
    "    if(subgraphs_score[i]>=top_score_value):\n",
    "        top_subgraphs.append(i)\n",
    "\n",
    "for i in top_subgraphs:\n",
    "    print(\"subgraph id: \", i, \" score: \", subgraphs_score[i])\n",
    "    print(\"pattern : \", extensions[i])\n",
    "    \n",
    "#print(sorted(filter_score, reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total correctly predicted images for class 1:  22\n",
      "\n",
      "\n",
      "\n",
      "subgraph id:  118  score:  6.0\n",
      "pattern :  [(0, 1, '11', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '18', '0'), (4, 5, '18', '18', '0'), (1, 6, '18', '18', '0'), (2, 3, '18', '16', '0')]\n",
      "subgraph id:  234  score:  7.0\n",
      "pattern :  [(0, 1, '16', '18', '0'), (1, 2, '18', '16', '0'), (1, 3, '18', '16', '0')]\n",
      "subgraph id:  240  score:  6.0\n",
      "pattern :  [(0, 1, '16', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '16', '0')]\n",
      "subgraph id:  241  score:  13.0\n",
      "pattern :  [(0, 1, '16', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '16', '0'), (2, 4, '18', '18', '0')]\n",
      "subgraph id:  245  score:  6.0\n",
      "pattern :  [(0, 1, '16', '18', '0'), (1, 2, '18', '18', '0'), (2, 3, '18', '18', '0'), (3, 4, '18', '16', '0'), (3, 5, '18', '18', '0')]\n",
      "subgraph id:  363  score:  10.0\n",
      "pattern :  [(0, 1, '16', '18', '0'), (1, 2, '18', '18', '0'), (1, 3, '18', '18', '0')]\n"
     ]
    }
   ],
   "source": [
    "# For class 1\n",
    "print(\"Total correctly predicted images for class 1: \", len(right_1_pred_idx))\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "subgraphs_score = np.zeros(len(extensions))\n",
    "for img in right_1_pred_idx:\n",
    "    s = method_to_task_to_scores['revealcancel'][1][img]\n",
    "    arr = s.ravel()\n",
    "    top_nth_threshold = max(sorted(arr, reverse=True)[n_task1],0.0) #can be done before smoothing the scores\n",
    "    thresholded_points = 1.0*(arr >= top_nth_threshold)\n",
    "    thresholded_points = thresholded_points.reshape(131, 131, 5) #why 5 channels?\n",
    "    imp_inp = thresholded_points*x_test[img]\n",
    "    imp_subgraphs = thresholded_points*address_test[img]\n",
    "    #top_ones = imp_subgraphs[np.nonzero(imp_subgraphs)]\n",
    "    top_ones = []\n",
    "    for channel in range(n_channels):\n",
    "        a = imp_subgraphs[:, :, channel].ravel()\n",
    "        nonzeros = a[np.nonzero(a)]\n",
    "        nonzeros = nonzeros.astype(int)\n",
    "        for x in nonzeros:\n",
    "                top_ones += h[img][channel][x-1]\n",
    "    top_ones = set(top_ones)      \n",
    "    for top in top_ones:\n",
    "        subgraphs_score[top-1] += 1\n",
    "    ix += 1\n",
    "    \n",
    "top_percent = 1\n",
    "top_score_value = np.percentile(subgraphs_score, 100 - top_percent)\n",
    "#print(top_filters_value)\n",
    "\n",
    "top_subgraphs = []\n",
    "for i in range(len(subgraphs_score)):\n",
    "    if(subgraphs_score[i]>=top_score_value):\n",
    "        top_subgraphs.append(i)\n",
    "\n",
    "for i in top_subgraphs:\n",
    "    print(\"subgraph id: \", i, \" score: \", subgraphs_score[i])\n",
    "    print(\"pattern : \", extensions[i])\n",
    "    \n",
    "#print(sorted(filter_score, reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.65      0.64        49\n",
      "           1       0.56      0.54      0.55        41\n",
      "\n",
      "    accuracy                           0.60        90\n",
      "   macro avg       0.60      0.59      0.59        90\n",
      "weighted avg       0.60      0.60      0.60        90\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(original, predictions))  #tp / (tp + fp) : precision\n",
    "                                                     #tp / (tp + fn) : recall\n",
    "                                                     #f1_score :  2 * (precision * recall) / (precision + recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#precision : out of predicted positives how many are true positives\n",
    "#recall : out of true positives how many are predicted positives\n",
    "\n",
    "#'macro':\n",
    "#Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\n",
    "\n",
    "#'weighted':\n",
    "#Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters macro to account for label imbalance; it can result in an F-score that is not between precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
